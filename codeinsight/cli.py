import argparse
import sys
from pathlib import Path
import libcst as cst

# æ ¸å¿ƒæ¨¡å—å¯¼å…¥
from codeinsight.refactor import UnusedImportRemover  # ç¡®ä¿ä½ å·²ç»åˆ›å»ºäº†è¿™ä¸ªæ–‡ä»¶
from .analyzer import CodeMetrics
from .cst_printer import print_cst_tree
from .multi_file_analyzer import MultiFileAnalyzer, ReportExporter
from .code_detector import CodeDuplicateDetector, ASTBasedDuplicateDetector, format_duplicate_report
from .evolution import EvolutionAnalyzer
from .checker import check_logic_bugs

def main():
    parser = argparse.ArgumentParser(description="CodeInsight: å¤šç»´åº¦ Python ä»£ç è´¨é‡åˆ†æå·¥å…·")
    parser.add_argument("file", help="Python æºæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--fix", action="store_true", help="è‡ªåŠ¨ä¿®å¤å¯å®‰å…¨ä¿®å¤çš„é—®é¢˜ï¼ˆç›®å‰æ”¯æŒï¼šç§»é™¤æœªä½¿ç”¨å¯¼å…¥ï¼‰")
    parser.add_argument("--show-cst", action="store_true", help="æ˜¾ç¤ºç®€åŒ–è¯­æ³•æ ‘")
    parser.add_argument("--show-functions", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†çš„å‡½æ•°åˆ†æ")
    parser.add_argument("--detect-duplicates", action="store_true", help="æ£€æµ‹ä»£ç é‡å¤")
    parser.add_argument("--duplicate-mode", choices=["block", "function"], default="block", help="é‡å¤æ£€æµ‹æ¨¡å¼: block(ä»£ç å—) æˆ– function(å‡½æ•°)")
    parser.add_argument("--directory", "-d", action="store_true", help="åˆ†æç›®å½•ä¸‹çš„æ‰€æœ‰Pythonæ–‡ä»¶")
    parser.add_argument("--json", "-j", metavar="OUTPUT_FILE", help="å¯¼å‡ºä¸ºJSONæ ¼å¼")
    parser.add_argument("--recursive", "-r", action="store_true", default=True, help="é€’å½’åˆ†æå­ç›®å½•")
    parser.add_argument("--evolution", action="store_true", help="åˆ†ææ–‡ä»¶çš„å†å²æ¼”åŒ–è¶‹åŠ¿")
    parser.add_argument("--check-bugs", action="store_true", help="æ‰§è¡Œæ·±åº¦é€»è¾‘ Bug æ‰«æ")
    args = parser.parse_args()

    filepath = Path(args.file)

    # å¤„ç†ç›®å½•åˆ†æ
    if args.directory or filepath.is_dir():
        _analyze_directory(filepath, args)
        return

    # å¤„ç†å•æ–‡ä»¶åˆ†æ
    if not filepath.exists() or filepath.suffix != ".py":
        print("é”™è¯¯: è¯·æä¾›æœ‰æ•ˆçš„ .py æ–‡ä»¶", file=sys.stderr)
        sys.exit(1)

    with open(filepath, "r", encoding="utf-8") as f:
        source = f.read()

    try:
        tree = cst.parse_module(source)
    except Exception as e:
        print(f"è§£æå¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)

    if args.check_bugs:
        bug_findings = check_logic_bugs(tree)
        print("\nğŸ› æ·±åº¦ Bug æ‰«æç»“æœ:")
        if not bug_findings:
            print("  âœ… æœªå‘ç°å¸¸è§é€»è¾‘ç¼ºé™·")
        for bug in bug_findings:
            print(f"  {bug}")

    if args.evolution:
        print("\nâ³ å†å²æ¼”åŒ–è½¨è¿¹ (è¿‡å»10ä¸ªç‰ˆæœ¬):")
        ea = EvolutionAnalyzer(".")
        history = ea.analyze_history(str(filepath))
        for entry in history:
            print(f"  [{entry['date']}] {entry['commit']} | è¯„åˆ†: {entry['score']} | å¤æ‚åº¦: {entry['complexity']}")

    # --- 1. æ‰§è¡Œåˆ†ææŒ‡æ ‡ ---
    metrics = CodeMetrics()
    result = metrics.analyze(tree, source)

    # --- 2. è‡ªåŠ¨åŒ–ä¿®å¤é€»è¾‘ ---
    if args.fix and result['unused_imports']:
        print(f"\nğŸ› ï¸  æ­£åœ¨æ‰§è¡Œè‡ªåŠ¨ä¿®å¤: {filepath.name}")
        
        # å®ä¾‹åŒ–é‡æ„å™¨
        fixer = UnusedImportRemover(set(result['unused_imports']))
        
        # è½¬æ¢è¯­æ³•æ ‘
        modified_tree = tree.visit(fixer)
        new_code = modified_tree.code
        
        # å¦‚æœä»£ç æœ‰å˜åŒ–ï¼Œå†™å›æ–‡ä»¶
        if new_code != source:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(new_code)
            print(f"âœ… å·²è‡ªåŠ¨ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥: {', '.join(result['unused_imports'])}")
            
            # ä¿®å¤åé‡æ–°åˆ†æä¸€æ¬¡ï¼Œä»¥ä¿è¯åç»­è¾“å‡ºçš„æŠ¥å‘Šæ˜¯åŸºäºæœ€æ–°ä»£ç çš„
            source = new_code
            tree = modified_tree
            result = metrics.analyze(tree, source)
        else:
            print("ğŸ’¡ æœªå‘ç°å¯è‡ªåŠ¨ä¿®å¤çš„å˜æ›´ã€‚")

    # --- 3. è¾“å‡ºæŠ¥å‘Š ---
    print(f"\nğŸ” ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š: {filepath}")
    print("-" * 40)

    # è´¨é‡è¯„åˆ†
    quality_score = result['quality_score']
    if quality_score >= 80:
        score_emoji = "â­"
    elif quality_score >= 60:
        score_emoji = "ğŸ‘"
    elif quality_score >= 40:
        score_emoji = "âš ï¸"
    else:
        score_emoji = "âŒ"
    print(f"{score_emoji} ä»£ç è´¨é‡è¯„åˆ†: {quality_score}/100")

    # ä»£ç è§„æ¨¡ç»Ÿè®¡
    print(f"\nğŸ“ˆ ä»£ç è§„æ¨¡:")
    print(f"  ğŸ“ æ€»è¡Œæ•°: {result['line_count']}")
    print(f"  ğŸ’¬ æ³¨é‡Šè¡Œæ•°: {result['comment_count']}")
    if result['line_count'] > 0:
        code_lines = result['line_count'] - result['comment_count']
        density = (code_lines / result['line_count'] * 100)
        print(f"  ğŸ“Š ä»£ç å¯†åº¦: {density:.1f}%")

    # å¤æ‚åº¦æŒ‡æ ‡
    print(f"\nâš™ï¸  å¤æ‚åº¦æŒ‡æ ‡:")
    print(f"  ğŸ“Š åœˆå¤æ‚åº¦: {result['cyclomatic_complexity']}")
    print(f"  ğŸ§® å‡½æ•°æ•°é‡: {result['function_count']}")
    print(f"  ğŸ›ï¸  ç±»æ•°é‡: {result['class_count']}")
    print(f"  ğŸ“¦ æœ€å¤§åµŒå¥—æ·±åº¦: {result['max_nesting_depth']}")

    # ç±»å‹æ³¨è§£è¦†ç›–ç‡
    print(f"\nğŸ·ï¸  ç±»å‹æ³¨è§£è¦†ç›–ç‡:")
    annotation_coverage = result['annotation_coverage']
    coverage_bar = "â–ˆ" * int(annotation_coverage // 10) + "â–‘" * (10 - int(annotation_coverage // 10))
    print(f"  {coverage_bar} {annotation_coverage:.1f}% ({result['total_functions']} ä¸ªå‡½æ•°)")
    print(f"  âŒ ç¼ºå°‘è¿”å›ç±»å‹æ³¨è§£: {result['functions_missing_return_annotation']}")
    print(f"  âŒ ç¼ºå°‘å‚æ•°ç±»å‹æ³¨è§£: {result['functions_missing_param_annotation']}")

    # å¯¼å…¥ç®¡ç†
    print(f"\nğŸ“¦ å¯¼å…¥ç®¡ç†:")
    unused = result['unused_imports']
    if args.fix and not unused:
        print(f"  âœ¨ æ‰€æœ‰æœªä½¿ç”¨å¯¼å…¥å·²æ¸…ç†å®Œæ¯•")
    else:
        print(f"  ğŸ—‘ï¸  æœªä½¿ç”¨å¯¼å…¥ ({len(unused)}): {', '.join(unused) if unused else 'æ— '}")

    # å‡½æ•°çº§åˆ«è¯¦ç»†åˆ†æ
    if args.show_functions and result['functions']:
        print(f"\nğŸ”¬ å‡½æ•°çº§åˆ«åˆ†æ ({len(result['functions'])} ä¸ªå‡½æ•°):")
        for func in result['functions']:
            print(f"  ğŸ“Œ {func.name}()")
            print(f"     å‚æ•°: {func.params_count} ä¸ª", end="")
            if func.params_without_annotation > 0:
                print(f" (ç¼ºå°‘æ³¨è§£: {func.params_without_annotation})", end="")
            print()
            print(f"     è¡Œæ•°: {func.lines_count} è¡Œ", end="")
            if func.lines_count > 50:
                print(" âš ï¸ (é•¿å‡½æ•°)", end="")
            print()
            print(f"     è¿”å›æ³¨è§£: {'âœ“' if func.has_return_annotation else 'âœ—'}", end="")
            print(f"  æ–‡æ¡£å­—ç¬¦ä¸²: {'âœ“' if func.has_docstring else 'âœ—'}")

    # è¯„ä¼°å»ºè®®
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    suggestions_count = 0

    if quality_score < 60:
        print("  ğŸš¨ ä»£ç è´¨é‡éœ€è¦æ”¹è¿›ï¼Œä»¥ä¸‹æ˜¯ä¼˜å…ˆçº§å»ºè®®:")

    if result['cyclomatic_complexity'] > 10:
        print("  âš ï¸  åœˆå¤æ‚åº¦è¿‡é«˜ (> 10)ï¼Œå»ºè®®æ‹†åˆ†é€»è¾‘")
        suggestions_count += 1
    if result['max_nesting_depth'] > 4:
        print("  âš ï¸  åµŒå¥—è¿‡æ·± (> 4)ï¼Œè€ƒè™‘æå‰ return æˆ–æå–å‡½æ•°")
        suggestions_count += 1
    if unused:
        print(f"  âš ï¸  å­˜åœ¨ {len(unused)} ä¸ªæœªä½¿ç”¨å¯¼å…¥ï¼Œå»ºè®®è¿è¡Œ --fix è‡ªåŠ¨æ¸…ç†")
        suggestions_count += 1
    if result['annotation_coverage'] < 50:
        print(f"  ğŸ·ï¸   ç±»å‹æ³¨è§£è¦†ç›–ç‡è¾ƒä½ ({annotation_coverage:.1f}%)ï¼Œå»ºè®®ä¸ºå…³é”®å‡½æ•°æ·»åŠ ç±»å‹æ³¨è§£")
        suggestions_count += 1
    
    # é•¿å‡½æ•°æç¤º
    long_functions = [f for f in result['functions'] if f.lines_count > 50]
    if long_functions:
        print(f"  ğŸ“ å‘ç° {len(long_functions)} ä¸ªé•¿å‡½æ•° (>50è¡Œ):")
        for func in long_functions[:3]:
            print(f"     - {func.name}(): {func.lines_count} è¡Œ")
        suggestions_count += 1

    if suggestions_count == 0:
        print("  âœ… ä»£ç è´¨é‡è‰¯å¥½ï¼Œä¿æŒå½“å‰ç¼–ç æ ‡å‡†")

    if args.show_cst:
        print("\nğŸŒ³ ç®€åŒ–è¯­æ³•æ ‘ (å‰3å±‚):")
        print_cst_tree(tree, max_depth=3)

    # ä»£ç é‡å¤æ£€æµ‹
    if args.detect_duplicates:
        print("\n" + "=" * 50)
        if args.duplicate_mode == "block":
            detector = CodeDuplicateDetector(
                min_block_size=5,
                similarity_threshold=0.85,
                ignore_comments=True,
                ignore_whitespace=True
            )
            report = detector.detect(source)
        else:
            detector = ASTBasedDuplicateDetector(min_function_size=5)
            report = detector.detect(tree, source)
        print(format_duplicate_report(report))

    # JSONå¯¼å‡º
    if args.json:
        ReportExporter.export_json(result, args.json)
        print(f"\nâœ… æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {args.json}")


def _analyze_directory(directory: Path, args):
    """åˆ†æç›®å½•ä¸‹çš„æ‰€æœ‰Pythonæ–‡ä»¶"""
    analyzer = MultiFileAnalyzer()
    
    # å¦‚æœå¼€å¯äº† --fixï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ‰«æç›®å½•æ—¶å¯¹æ¯ä¸ªæ–‡ä»¶è¿›è¡Œå¤„ç†
    #ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œå‡è®¾ MultiFileAnalyzer å°šæœªé›†æˆä¿®å¤åŠŸèƒ½
    # å¦‚æœè¦åœ¨ç›®å½•æ‰«æä¸­ä¹Ÿæ”¯æŒ --fixï¼Œå»ºè®®åœ¨ MultiFileAnalyzer.analyze_directory ä¸­å®ç°é€»è¾‘
    result = analyzer.analyze_directory(str(directory), recursive=args.recursive)

    print(f"ğŸ“‚ ç›®å½•åˆ†ææŠ¥å‘Š: {result['directory']}\n")
    print(f"ğŸ“Š æ‰«æç»“æœ:")
    print(f"  æ€»æ–‡ä»¶æ•°: {result['total_files']}")
    print(f"  å·²åˆ†æ: {result['analyzed_files']}")

    summary = result['summary']
    if summary:
        print(f"\nğŸ“ˆ é¡¹ç›®çº§æ±‡æ€»:")
        print(f"  å¹³å‡è¯„åˆ†: {summary.get('average_quality_score', 0)}/100")
        print(f"  æœ€ä½³æ–‡ä»¶: {Path(summary.get('best_file', '')).name} ({summary.get('best_file_score', 0)})")
        print(f"  æœ€å·®æ–‡ä»¶: {Path(summary.get('worst_file', '')).name} ({summary.get('worst_file_score', 0)})")
        print(f"  æ€»å‡½æ•°æ•°: {summary.get('total_functions', 0)}")
        print(f"  æ€»ç±»æ•°: {summary.get('total_classes', 0)}")
        print(f"  æ€»è¡Œæ•°: {summary.get('total_lines', 0)}")

    # æŒ‰è¯„åˆ†æ’åºå¹¶æ˜¾ç¤º
    print(f"\nğŸ“‹ æ–‡ä»¶è´¨é‡æ’åº:")
    files_with_scores = [
        (f, r['quality_score']) for f, r in result['files'].items()
        if isinstance(r, dict) and 'quality_score' in r
    ]
    files_with_scores.sort(key=lambda x: x[1], reverse=True)

    for file_path, score in files_with_scores[:10]:
        emoji = "â­" if score >= 80 else "ğŸ‘" if score >= 60 else "âš ï¸" if score >= 40 else "âŒ"
        print(f"  {emoji} {Path(file_path).name}: {score}/100")

    if args.json:
        ReportExporter.export_json(result, args.json)
        print(f"\nâœ… æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {args.json}")


if __name__ == "__main__":
    main()
